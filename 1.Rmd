---
title: "Data Science Homework Data Analysis Project"
author: "Kai Yip Lei (Derek)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  prettydoc::html_pretty:
  toc : true
  toc_float : true

---

## 1. Setup, Loading libraries and Data  
  
This task is about default payments in Taiwan and our goal is to predict the probability of default, I started the task by loading the libraries and data.  
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(RCurl)
library(rmarkdown)
library(knitr)
library(stats4)
library(stats)
library(pander)
library(NbClust)
library(randomForest)
library(ROCR)
library(glmnet)
library(e1071)
library(doParallel)
library(data.table)
library(tidyr)
library(gridExtra)

rm(list=ls())
dr<-data.table(read.csv("default of credit card clients.csv",header=TRUE,sep=",",skip=1)%>%rename(y=default.payment.next.month))
set.seed(0706)

opts_chunk$set(cache=TRUE)
options(digit=4)
```
  
The original dataset has `r nrow(dr)` observations and `r ncol(dr)-1` features, it can be downloaded from [here](http://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients).
The response variable **Default Payment Next Month** was renamed to **y** for convenience and the seed was set to **0706** here to ensure reproducibility.  
  
## 2. Exploratory data analysis & Feature Engineering  
  
### a. Data description  

The variables in the dataset are defined as follows:  

* __default.payment.next.month__ : default payment next month(Yes = 1, No = 0).  
* __ID__ : ID of the individual consumer.  
* __LIMIT_BAL__ : Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.  
* __SEX__ : Gender (1 = male; 2 = female).  
* __EDUCATION__ : Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).  
* __MARRIAGE__ : Marital status (1 = married; 2 = single; 3 = others).  
* __AGE__ : Age (year).  
* __PAY_0__ - __PAY_6__ : History of past payment. Past monthly payment records. __PAY_0__ = the repayment status in September, 2005; __PAY_2__ = the repayment status in August, 2005; . . .; __PAY_6__ = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.  
* __BILL_AMT1__ - __BILL_AMT6__ : Amount of bill statement (NT dollar). __BILL_AMT1__ = amount of bill statement in September, 2005; __BILL_AMT2__ = amount of bill statement in August, 2005; . . .; __BILL_AMT6__ = amount of bill statement in April, 2005.
* __PAY_AMT1__ - __PAY_AMT6__ : Amount of previous payment (NT dollar). __PAY_AMT1__ = amount paid in September, 2005; __PAY_AMT2__ = amount paid in August, 2005; . . .; __PAY_AMT6__ = amount paid in April, 2005.  
  
### b. Exploratory data analysis  
I manipulated the data a bit here before doing any analysis. The variable **ID** is dropped because it is meaningless and unique to every individual. Variables **SEX**, **EDUCATION** and **MARRIAGE** are converted to factors because they should be categorical instead of numerical.  
```{r results="hide", message=FALSE, warning=FALSE}
d<-data.table(dr)
d[,ID:=NULL]
d[,SEX:=factor(SEX,levels=c("1","2"),labels=c("Male","Female"))]
d[,EDUCATION:=as.factor(EDUCATION)]
d[,MARRIAGE:=as.factor(MARRIAGE)]

```
  
The following graphs are the histograms of the variable **SEX**, **EDUCATION** and **MARRIAGE**. The variable **SEX** looks slightly skewed, having more female than male. Although most of the observations are from individuals with high school or higher education and either single or married, a few of them are in the others category for education or marriage. Note that there are unknown education and marriage category that are not described in the dataset nor the research paper, luckily they only consist a very small amount of the total data so it should not affect my analysis.  
```{r fig.cap="Histograms of **SEX**, **EDUCATION** and **MARRIAGE**", message=FALSE, warning=FALSE}

ggplot(data.table(gather(d[,.(SEX,EDUCATION,MARRIAGE)],K,V)))+geom_bar(aes(x=V,fill=V))+facet_wrap(~K,scales="free")+
  xlab("")+ylab("Count")+scale_fill_brewer(palette = "Spectral",guide=FALSE)
  
```
  
In addition, the following graphs show the heatmap of the probability of having a credit default next month for different type of individuals, a warmer color indicates a higher default probability.  
```{r fig.cap="Heatmap of the probability of credit default", message=FALSE, warning=FALSE}

ggplot(d[EDUCATION%in%c("1","2","3","4")&MARRIAGE%in%c("1","2","3"),mean(y),by=.(SEX,EDUCATION,MARRIAGE)],aes(x=EDUCATION,y=MARRIAGE))+geom_tile(aes(fill=V1))+facet_wrap(~SEX)+
  scale_fill_distiller(palette="Spectral","Probability")
```
  
We can see that individuals with higher education are less likely to have credit default, except for post-graduate individuals with marital status of 3 (probably divorced). Another observation is that female individuals are generally less likely to have default compared to male individuals, it implies that men and women may have different patterns of paying bills and gender might be one of the important variables in the analysis.  
  
***  
  
Since Payment status variables are not continuous, I converted them to factors as well. (For example, status -1 means that the individual paid duly, which is not comparable with the individual paid 1 month late.)  
```{r}
d$PAY_0<-as.factor(d$PAY_0)
d$PAY_2<-as.factor(d$PAY_2)
d$PAY_3<-as.factor(d$PAY_3)
d$PAY_4<-as.factor(d$PAY_4)
d$PAY_5<-as.factor(d$PAY_5)
d$PAY_6<-as.factor(d$PAY_6)
```
The follow graphs are the histograms of payment status 0-6.  
```{r fig.cap="Histograms of payment status **PAY_0** - **PAY_6**", message=FALSE, warning=FALSE}

ggplot(data.table(gather(d[,.(PAY_0,PAY_2,PAY_3,PAY_4,PAY_5,PAY_6)],K,V)))+geom_bar(aes(x=V,fill=log(..count..)))+facet_wrap(~K)+scale_x_discrete(limits=sort(d[,.N,by=.(PAY_0)]$PAY_0))+scale_fill_distiller(palette="Spectral",guide=FALSE)+
  xlab("Payment Status")+ylab("Count")
```
  
Most of the individuals pay a little late but less than 1 month. An interesting observation is that there are more people paying late for 2 months than for 1 month.  
  
***  
  
The follow figures are the histograms of bill amount 1-6, in both real amount and log amount.  
```{r fig.cap=c("Histograms of bill amount **BILL_AMT1** - **BILL_AMT6**","Histograms of log bill amount **BILL_AMT1** - **BILL_AMT6**"), message=FALSE, warning=FALSE}

ggplot(data.table(gather(d[,.(BILL_AMT1,BILL_AMT2,BILL_AMT3,BILL_AMT4,BILL_AMT5,BILL_AMT6)],K,V)))+geom_histogram(aes(x=V,fill=log(..count..)))+
  facet_wrap(~K)+scale_fill_distiller(palette="Spectral",guide=FALSE)+xlab("Bill amount")+ylab("Count")

ggplot(data.table(gather(d[,.(BILL_AMT1,BILL_AMT2,BILL_AMT3,BILL_AMT4,BILL_AMT5,BILL_AMT6)],K,V))[V>0])+geom_histogram(aes(x=log(V),fill=log(..count..)))+
  facet_wrap(~K)+scale_fill_distiller(palette="Spectral",guide=FALSE)+xlab("Log bill amount")+ylab("Count")
```
  
From the figures above, we can see that bill amounts can be negative here, possibly due to over payment. The bill amounts are very skewed and have long right tails, the log amounts are less skewed but instead have long left tails.  
  
We do the same for payment amounts, the follow figures are the histograms of payment amount 1-6, in both real amount and log amount. 
```{r fig.cap=c("Histograms of payment amount **PAY_AMT1** - **PAY_AMT6**","Histograms of log bill amount **PAY_AMT1** - **PAY_AMT6**"), message=FALSE, warning=FALSE}
ggplot(data.table(gather(d[,.(PAY_AMT1,PAY_AMT2,PAY_AMT3,PAY_AMT4,PAY_AMT5,PAY_AMT6)],K,V)))+geom_histogram(aes(x=V,fill=log(..count..)))+
  facet_wrap(~K)+scale_fill_distiller(palette="Spectral",guide=FALSE)+xlab("Payment amount")+ylab("Count")

ggplot(data.table(gather(d[,.(PAY_AMT1,PAY_AMT2,PAY_AMT3,PAY_AMT4,PAY_AMT5,PAY_AMT6)],K,V))[V>0])+geom_histogram(aes(x=log(V),fill=log(..count..)))+
  facet_wrap(~K)+scale_fill_distiller(palette="Spectral",guide=FALSE)+xlab("Log payment amount")+ylab("Count")
```
  
The payment amounts cannot be negative here, but again the bill amounts are very skewed and have long right tails, the log amounts this time are much less skewed.   
  
We investigate further using the following graphs.  
```{r fig.cap=c("Heatmap of total bill amount against total payment amount","Default probability against differece of log bill and payment amount"), message=FALSE, warning=FALSE}
ggplot(d[(BILL_AMT2+BILL_AMT3+BILL_AMT4+BILL_AMT5+BILL_AMT6>0)&(PAY_AMT1+PAY_AMT2+PAY_AMT3+PAY_AMT4+PAY_AMT5>0)])+geom_hex(aes(x=log(BILL_AMT2+BILL_AMT3+BILL_AMT4+BILL_AMT5+BILL_AMT6),y=log(PAY_AMT1+PAY_AMT2+PAY_AMT3+PAY_AMT4+PAY_AMT5)))+
  scale_fill_distiller(palette="Spectral")+xlab("Log Bill amount from Apr to Aug")+ylab("Log Payment amount from May to Sep")+geom_abline(col="#00BFC4")

ggplot(d)+
  geom_smooth(se=F,aes(x=log(BILL_AMT2)-log(PAY_AMT1),y=y,col="Aug 2005"))+
  geom_smooth(se=F,aes(x=log(BILL_AMT3)-log(PAY_AMT2),y=y,col="Jul 2005"))+
  geom_smooth(se=F,aes(x=log(BILL_AMT4)-log(PAY_AMT3),y=y,col="Jun 2005"))+
  geom_smooth(se=F,aes(x=log(BILL_AMT5)-log(PAY_AMT4),y=y,col="May 2005"))+
  geom_smooth(se=F,aes(x=log(BILL_AMT6)-log(PAY_AMT5),y=y,col="Apr 2005"))+
  xlab("Difference of log bill and log payment")+ylab("Default probability")+
  scale_color_brewer(palette="Spectral","Month")
```
  
The first graph is a heatmap of bill amount against payment amount, a significant number of observations lie on the 45 degree line and those individuals probably always pay the exact bill amount. A lot of people pay less than the bill amount and there is a blue horizontal segment at around log payment amount equals to 6, these people may represent individuals that only pay the minimum required payment.  
  
The second graph shows the relation of default probability and the difference of log bill and log payment amount. Default probability is lowest when the difference is almost 0, which means that those who pay the exact bill amount are less likely to have credit default next month. while it is obvious that default probability is high when difference of log bill and log payment is positive, it is high when the difference is negative as well, those customers probably paid more because they have overdue from previous months.  
  
***  
  
The final variable is the credit limit, the following graphs are the histogram and its relation with the default probability.  
```{r fig.cap=c("Histogram of **LIMIT_BAL**","Default probability against **LIMIT_BAL**"), message=FALSE, warning=FALSE}
ggplot(d)+geom_histogram(aes(x=LIMIT_BAL,fill=..count..))+scale_fill_distiller(palette="Spectral")

ggplot(d[,mean(y),by=(round(exp(round(log(d$LIMIT_BAL),2))))])+geom_smooth(aes(x=round,y=V1),se=F)+
  xlab("Amount of the given credit (NT dollar)")+ylab("Default probability")
```
  
Credit limit has a very significant negative correlation with default probability, which makes sense because banks only give high limit to those who always pay duly.  
  
Since **LIMIT_BAL**, **BILL_AMT** and **PAY_AMT** are skewed, log variables of these variables are also created, although it would not affect tree-based models, it is still useful for GLM. For negative values or zeros, flags are also created to distinguish these observations.  
```{r, message=FALSE, warning=FALSE}
d$LLIMIT_BAL<-log(d$LIMIT_BAL)
d$LBILL_AMT1<-ifelse(d$BILL_AMT1>0,log(d$BILL_AMT1),0)
d$LBILL_AMT2<-ifelse(d$BILL_AMT2>0,log(d$BILL_AMT2),0)
d$LBILL_AMT3<-ifelse(d$BILL_AMT3>0,log(d$BILL_AMT3),0)
d$LBILL_AMT4<-ifelse(d$BILL_AMT4>0,log(d$BILL_AMT4),0)
d$LBILL_AMT5<-ifelse(d$BILL_AMT5>0,log(d$BILL_AMT5),0)
d$LBILL_AMT6<-ifelse(d$BILL_AMT6>0,log(d$BILL_AMT6),0)
d$LPAY_AMT1<-ifelse(d$PAY_AMT1>0,log(d$PAY_AMT1),0)
d$LPAY_AMT2<-ifelse(d$PAY_AMT2>0,log(d$PAY_AMT2),0)
d$LPAY_AMT3<-ifelse(d$PAY_AMT3>0,log(d$PAY_AMT3),0)
d$LPAY_AMT4<-ifelse(d$PAY_AMT4>0,log(d$PAY_AMT4),0)
d$LPAY_AMT5<-ifelse(d$PAY_AMT5>0,log(d$PAY_AMT5),0)
d$LPAY_AMT6<-ifelse(d$PAY_AMT6>0,log(d$PAY_AMT6),0)

d$BILL_AMT1_0<-ifelse(d$BILL_AMT1==0,1,0)
d$BILL_AMT2_0<-ifelse(d$BILL_AMT2==0,1,0)
d$BILL_AMT3_0<-ifelse(d$BILL_AMT3==0,1,0)
d$BILL_AMT4_0<-ifelse(d$BILL_AMT4==0,1,0)
d$BILL_AMT5_0<-ifelse(d$BILL_AMT5==0,1,0)
d$BILL_AMT6_0<-ifelse(d$BILL_AMT6==0,1,0)

d$BILL_AMT1_N<-ifelse(d$BILL_AMT1<0,1,0)
d$BILL_AMT2_N<-ifelse(d$BILL_AMT2<0,1,0)
d$BILL_AMT3_N<-ifelse(d$BILL_AMT3<0,1,0)
d$BILL_AMT4_N<-ifelse(d$BILL_AMT4<0,1,0)
d$BILL_AMT5_N<-ifelse(d$BILL_AMT5<0,1,0)
d$BILL_AMT6_N<-ifelse(d$BILL_AMT6<0,1,0)

d$PAY_AMT1_0<-ifelse(d$PAY_AMT1==0,1,0)
d$PAY_AMT2_0<-ifelse(d$PAY_AMT2==0,1,0)
d$PAY_AMT3_0<-ifelse(d$PAY_AMT3==0,1,0)
d$PAY_AMT4_0<-ifelse(d$PAY_AMT4==0,1,0)
d$PAY_AMT5_0<-ifelse(d$PAY_AMT5==0,1,0)
d$PAY_AMT6_0<-ifelse(d$PAY_AMT6==0,1,0)
```
  
## 3. Predictive Analysis  

I decided to use H2O because its standardized syntax, it also supports grid search natively which can be useful in the following analysis. I start up H2O using 6 threads, and then removed all existing data in H2O.  

```{r message=FALSE, warning=FALSE, results="hide",cache=FALSE}
library(h2o)
h2o.init(nthreads = 6)
h2o.removeAll()
```
  
Since my response variable is also a categorical value, I converted it to a factor and upload the whole dataset to H2O. Using the *h2o.splitFrame* function, I splitted the dataset into three parts in a ratio of 6:2:2, and named them train, valid and test respectively. I also removed some strange values of the **EDUCATION** and the **MARRIAGE**.  
```{r message=FALSE, warning=FALSE, results="hide",cache=FALSE}
df<-d
df[,y:=as.factor(y)]
df[,PAY_0:=as.factor(PAY_0)]
df[,PAY_2:=as.factor(PAY_2)]
df[,PAY_3:=as.factor(PAY_3)]
df[,PAY_4:=as.factor(PAY_4)]
df[,PAY_5:=as.factor(PAY_5)]
df[,PAY_6:=as.factor(PAY_6)]
df<-df[EDUCATION%in%c(1,2,3,4)&MARRIAGE%in%c(1,2,3)]
h2o_d<-as.h2o(df)
h2o_sd<- h2o.splitFrame(h2o_d, ratios = c(.6, 0.2),seed=0706)
names(h2o_sd) <- c("train", "valid", "test")
```

After splitting, there are `r attributes(h2o_sd$train)$nrow`, `r attributes(h2o_sd$valid)$nrow` and `r attributes(h2o_sd$test)$nrow` observations in the training set, validation set and test set respectively and each with `r ncol(df)` features. In the following analysis, I will start with Naive Bayes as a benchmark, and compare the performance of GLM (Elastic Net regularized logit regression), Random Forest and finally Gradient Machine Boosting.  
  
### a. Naive Bayes Classifier  
Here I used the function *h2o.naiveBayes* to predict the credit default using Naive Bayes Classifier, using **y** as the response variable, all other variables as features. Note that the seed here is not really doing anything but just to standardize the code because I am going to use the same seed in other models.  
```{r message=FALSE, warning=FALSE, results="hide",cache=FALSE}
NB<-h2o.naiveBayes(
  training_frame = h2o_sd$train,
  validation_frame = h2o_sd$valid,
  x=colnames(h2o_sd$train)[-24],
  y="y",
  seed=0706
)
NBp<-h2o.performance(NB)
h2o.auc(NBp)
NBrp<-cbind(h2o.fpr(NBp),h2o.tpr(NBp)$tpr)
colnames(NBrp)[3]<-"tpr"
NBt<-h2o.performance(NB,newdata = h2o_sd$test)
NBrt<-cbind(h2o.fpr(NBt),h2o.tpr(NBt)$tpr)
h2o.auc(NBt)
colnames(NBrt)[3]<-"tpr"
```
  
I extract the performance metrics from the model and use the model to predict outcomes in the test set. Overall I get an AUC of `r round(h2o.auc(NBp),4)` for the validation set and an AUC of `r round(h2o.auc(NBt),4)` for the test set, which is pretty good given the simplicity of the model.  
  
I am going to use the F2 score to decide the threshold in all models since it places more emphasis on False Negative, which is better in this scenario because a bank possibly prefer to incorrectly predict individuals to default than to have unexpected defaults. 

```{r  cache=FALSE}
NBev<-ggplot(h2o.F2(NBp))+geom_line(aes(x=threshold,y=f2,color=threshold),size=1)+
  scale_color_gradient2("Threshold",low="red",high="green",mid="yellow",midpoint = 0.5)+
  xlab("Threshold")+ylab("F2 Score")+xlim(0,1)+
  geom_point(size=2,aes(x=h2o.find_threshold_by_max_metric(NBp,"f2"),y=h2o.F2(NBp,h2o.find_threshold_by_max_metric(NBp,"f2"))[[1]]),col="blue")
NBet<-ggplot(h2o.F2(NBt))+geom_line(aes(x=threshold,y=f2,color=threshold),size=1)+
  scale_color_gradient2("Threshold",low="red",high="green",mid="yellow",midpoint = 0.5)+
  xlab("Threshold")+ylab("F2 Score")+xlim(0,1)+
  geom_point(size=2,aes(x=h2o.find_threshold_by_max_metric(NBt,"f2"),y=h2o.F2(NBt,h2o.find_threshold_by_max_metric(NBt,"f2"))[[1]]),col="blue")
NBav<-ggplot(NBrp,aes(x=fpr,y=tpr))+geom_line(aes(col=threshold),size=1)+xlab("False Positive Rate")+ylab("True Positive Rate")+
  scale_color_gradient2("Threshold",low="red",high="green",mid="yellow",midpoint = 0.5)+
  geom_segment(x=0,y=0,xend=1,yend=1,size=1,col="#00BFC4")+xlim(0,1)+ylim(0,1)+
  geom_point(size=2,aes(x=h2o.fpr(NBp,h2o.find_threshold_by_max_metric(NBp,"f2"))[[1]],y=h2o.tpr(NBp,h2o.find_threshold_by_max_metric(NBp,"f2"))[[1]]),col="blue")
NBat<-ggplot(NBrt,aes(x=fpr,y=tpr))+geom_line(aes(col=threshold),size=1)+xlab("False Positive Rate")+ylab("True Positive Rate")+
  scale_color_gradient2("Threshold",low="red",high="green",mid="yellow",midpoint = 0.5)+
  geom_segment(x=0,y=0,xend=1,yend=1,size=1,col="#00BFC4")+xlim(0,1)+ylim(0,1)+
  geom_point(size=2,aes(x=h2o.fpr(NBt,h2o.find_threshold_by_max_metric(NBt,"f2"))[[1]],y=h2o.tpr(NBt,h2o.find_threshold_by_max_metric(NBt,"f2"))[[1]]),col="blue")
```
  
The following two graphs show the metrics and ROC curve with different threshold for validation set and test set, both figures use a warmer color for lower threshold.  

```{r fig.cap="F2-Threshold curve and ROC curve for the **validation set**", cache=FALSE}
grid.arrange(NBev,NBav,ncol=1)
```
```{r fig.cap="F2-Threshold curve and ROC curve for the **test set**", cache=FALSE}
grid.arrange(NBet,NBat,ncol=1)
```
  
The blue dots on the graph indicate the threshold chosen and the confusion matrices are as follows:  
  
**Validation set**:  
```{r  cache=FALSE}
pander(h2o.confusionMatrix(NBp,metrics="f2")[,1:3])
```

**Test set**:  
```{r  cache=FALSE}
pander(h2o.confusionMatrix(NBt,metrics="f2")[,1:3])
```
  
The rows are the actual value and the columns are the prediction values. In the prediction, we failed to identify `r round(h2o.confusionMatrix(NBt,metrics="f2")[2,3]*100,2)`% of the credit default, which we will use it as a benchmark for the following analysis. Note that we are going to have a high class error for the non-default class because we prefer false positive than false negative, the class error here is `r round(h2o.confusionMatrix(NBt,metrics="f2")[1,3]*100,2)`%.  
  
### b. Logit Classifier with regularization (LASSO, ridge and Elastic Net)  
  
Here I try to apply GLM using logit regression with different regularization, I defined a grid **GLM** in H2O, then run grid search using different alpha and use max lambda search to optimize. (alpha = 0 when ridge, and alpha = 1 when LASSO)  

```{r message=FALSE, warning=FALSE, results="hide",cache=FALSE}
h2o.rm("GLM")
GLM <- h2o.grid(
  algorithm = "glm", 
  grid_id = "GLM",
  hyper_params = list(alpha = c(0,0.25,0.5,0.75,1)),
  training_frame = h2o_sd$train,
  validation_frame = h2o_sd$valid,
  x=colnames(h2o_sd$train)[-24],
  y="y",
  seed=0706,
  family = "binomial",
  lambda_search =T
)

GLMm<-h2o.getGrid(
  grid_id = "GLM", 
  sort_by = "F2",
  decreasing = TRUE
)
GLMb<- h2o.getModel(GLMm@model_ids[[1]])
```
  
Again I chose the model with highest F2, which is the model with alpha equals to `r GLMb@parameters$alpha`. I also predict the values for the test set using the best model.    
```{r message=FALSE, warning=FALSE, results="hide",cache=FALSE}
GLMp<-h2o.performance(GLMb,valid=T)
h2o.auc(GLMp)
GLMrp<-cbind(h2o.fpr(GLMp),h2o.tpr(GLMp)$tpr)
colnames(GLMrp)[3]<-"tpr"
GLMt<-h2o.performance(GLMb,newdata = h2o_sd$test,xval=T)
GLMrt<-cbind(h2o.fpr(GLMt),h2o.tpr(GLMt)$tpr)
h2o.auc(GLMt)
colnames(GLMrt)[3]<-"tpr"
```
  
I get an AUC of `r round(h2o.auc(GLMp),4)` for the validation set and an AUC of `r round(h2o.auc(GLMt),4)` for the test set, which are both better than the NB predictor. The following two graphs are the F2 to threshold curve and the ROC curve, the blue dots again indicate the threshold chosen.  
```{r, cache=FALSE}
GLMev<-ggplot(h2o.F2(GLMp))+geom_line(aes(x=threshold,y=f2,color=threshold),size=1)+
  scale_color_gradient2("Threshold",low="red",high="green",mid="yellow",midpoint = 0.5)+
  xlab("Threshold")+ylab("F2 Score")+xlim(0,1)+
  geom_point(size=2,aes(x=h2o.find_threshold_by_max_metric(GLMp,"f2"),y=h2o.F2(GLMp,h2o.find_threshold_by_max_metric(GLMp,"f2"))[[1]]),col="blue")
GLMet<-ggplot(h2o.F2(GLMt))+geom_line(aes(x=threshold,y=f2,color=threshold),size=1)+
  scale_color_gradient2("Threshold",low="red",high="green",mid="yellow",midpoint = 0.5)+
  xlab("Threshold")+ylab("F2 Score")+xlim(0,1)+
  geom_point(size=2,aes(x=h2o.find_threshold_by_max_metric(GLMt,"f2"),y=h2o.F2(GLMt,h2o.find_threshold_by_max_metric(GLMt,"f2"))[[1]]),col="blue")
GLMav<-ggplot(GLMrp,aes(x=fpr,y=tpr))+geom_line(aes(col=threshold),size=1)+xlab("False Positive Rate")+ylab("True Positive Rate")+
  scale_color_gradient2("Threshold",low="red",high="green",mid="yellow",midpoint = 0.5)+
  geom_segment(x=0,y=0,xend=1,yend=1,size=1,col="#00BFC4")+xlim(0,1)+ylim(0,1)+
  geom_point(size=2,aes(x=h2o.fpr(GLMp,h2o.find_threshold_by_max_metric(GLMp,"f2"))[[1]],y=h2o.tpr(GLMp,h2o.find_threshold_by_max_metric(GLMp,"f2"))[[1]]),col="blue")
GLMat<-ggplot(GLMrt,aes(x=fpr,y=tpr))+geom_line(aes(col=threshold),size=1)+xlab("False Positive Rate")+ylab("True Positive Rate")+
  scale_color_gradient2("Threshold",low="red",high="green",mid="yellow",midpoint = 0.5)+
  geom_segment(x=0,y=0,xend=1,yend=1,size=1,col="#00BFC4")+xlim(0,1)+ylim(0,1)+
  geom_point(size=2,aes(x=h2o.fpr(GLMt,h2o.find_threshold_by_max_metric(GLMt,"f2"))[[1]],y=h2o.tpr(GLMt,h2o.find_threshold_by_max_metric(GLMt,"f2"))[[1]]),col="blue")

```

```{r fig.cap="F2-Threshold curve and ROC curve for the **validation set**", cache=FALSE}
grid.arrange(GLMev,GLMav,ncol=1)
```
```{r fig.cap="F2-Threshold curve and ROC curve for the **test set**", cache=FALSE}
grid.arrange(GLMet,GLMat,ncol=1)
```
  
The confusion matrices are as follows:  
**Validation set**:  
```{r cache=FALSE}
pander(h2o.confusionMatrix(GLMp,metrics="f2")[,1:3])
```

**Test set**:  
```{r cache=FALSE}
pander(h2o.confusionMatrix(GLMt,metrics="f2")[,1:3])
```
We failed to identify `r round(h2o.confusionMatrix(GLMt,metrics="f2")[2,3]*100,2)`% of the credit default in the test set, which is better than the NB classifier, but the class error of not defaulting is `r round(h2o.confusionMatrix(GLMt,metrics="f2")[1,3]*100,2)`%, worse than the NB predictor.  
  
I also plotted the graph showing the relative magnitude of the variables using color to indicate the sign of the coefficient.  
```{r fig.cap="Relative magnitude of 10 most important variables"}
ggplot(data.table(cbind(h2o.varimp(GLMb)$names[1:10],h2o.varimp(GLMb)$coefficients[1:10],h2o.varimp(GLMb)$sign[1:10]))[order(-V2)])+
  geom_col(aes(x=V1,y=V2,fill=V3))+coord_flip()+scale_x_discrete(limits=rev(h2o.varimp(GLMb)$names[1:10]))+
  theme(axis.text.x=element_blank(),axis.ticks=element_blank())+ylab("Magnitude")+xlab("Variable")+
  scale_fill_discrete("Sign",labels=c("Negative","Positive"))
```
  
From the graph above, we can see almost all the most important variables are payment status. The first three variables actually make sense because if you pay duly or just a little late, you are probably going to pay on time next time, but you are more likely to default if you for example are 2 months late. The education 4 is important as well, but since it is in the other category, not much can be interpreted.  
  
### c. Random Forest  
Here I try to apply Random Forest, I defined a grid **RF** in H2O, then run grid search using different max tree depth and variables each tree, then I choose the best model with highest F2 score.  
```{r results="hide", message=FALSE, warning=FALSE,cache=FALSE}
h2o.rm("RF")

RF <- h2o.grid(
  algorithm = "randomForest", 
  grid_id = "RF",
  hyper_params = list(max_depth=c(15,20,25),mtries=c(5,7,9)),
  training_frame = h2o_sd$train,
  validation_frame = h2o_sd$valid,
  x=colnames(h2o_sd$train)[-24],
  y="y",
  seed=0706,
  ntrees=100
)
RFm<-h2o.getGrid(
  grid_id = "RF", 
  sort_by = "F2",
  decreasing = TRUE
)

RFb<- h2o.getModel(RFm@model_ids[[1]])
```
  
The best model is the model with `r RFb@parameters$max_depth` as max tree depth and `r RFb@parameters$mtries` as variables each tree.  
  
I then extract the performance and do prediction on the test set.  
```{r message=FALSE, warning=FALSE, results="hide",cache=FALSE}
RFp<-h2o.performance(RFb,valid=T)
h2o.auc(RFp)
RFrp<-cbind(h2o.fpr(RFp),h2o.tpr(RFp)$tpr)
colnames(RFrp)[3]<-"tpr"
RFt<-h2o.performance(RFb,newdata = h2o_sd$test)
RFrt<-cbind(h2o.fpr(RFt),h2o.tpr(RFt)$tpr)
h2o.auc(RFt)
colnames(RFrt)[3]<-"tpr"
```
  
I get an AUC of `r round(h2o.auc(RFp),4)` for the validation set and an AUC of `r round(h2o.auc(RFt),4)` for the test set, which are both better than the NB predictor, the AUC for validation set is slightly lower than that of GLM but the AUC for test set is higher. The following two graphs are the F2 to threshold curve and the ROC curve, the blue dots again indicate the threshold chosen.  

```{r cache=FALSE}
RFev<-ggplot(h2o.F2(RFp))+geom_line(aes(x=threshold,y=f2,color=threshold),size=1)+
  scale_color_gradient2("Threshold",low="red",high="green",mid="yellow",midpoint = 0.5)+
  xlab("Threshold")+ylab("F2 Score")+xlim(0,1)+
  geom_point(size=2,aes(x=h2o.find_threshold_by_max_metric(RFp,"f2"),y=h2o.F2(RFp,h2o.find_threshold_by_max_metric(RFp,"f2"))[[1]]),col="blue")
RFet<-ggplot(h2o.F2(RFt))+geom_line(aes(x=threshold,y=f2,color=threshold),size=1)+
  scale_color_gradient2("Threshold",low="red",high="green",mid="yellow",midpoint = 0.5)+
  xlab("Threshold")+ylab("F2 Score")+xlim(0,1)+
  geom_point(size=2,aes(x=h2o.find_threshold_by_max_metric(RFt,"f2"),y=h2o.F2(RFt,h2o.find_threshold_by_max_metric(RFt,"f2"))[[1]]),col="blue")
RFav<-ggplot(RFrp,aes(x=fpr,y=tpr))+geom_line(aes(col=threshold),size=1)+xlab("False Positive Rate")+ylab("True Positive Rate")+
  scale_color_gradient2("Threshold",low="red",high="green",mid="yellow",midpoint = 0.5)+
  geom_segment(x=0,y=0,xend=1,yend=1,size=1,col="#00BFC4")+xlim(0,1)+ylim(0,1)+
  geom_point(size=2,aes(x=h2o.fpr(RFp,h2o.find_threshold_by_max_metric(RFp,"f2"))[[1]],y=h2o.tpr(RFp,h2o.find_threshold_by_max_metric(RFp,"f2"))[[1]]),col="blue")
RFat<-ggplot(RFrt,aes(x=fpr,y=tpr))+geom_line(aes(col=threshold),size=1)+xlab("False Positive Rate")+ylab("True Positive Rate")+
  scale_color_gradient2("Threshold",low="red",high="green",mid="yellow",midpoint = 0.5)+
  geom_segment(x=0,y=0,xend=1,yend=1,size=1,col="#00BFC4")+xlim(0,1)+ylim(0,1)+
  geom_point(size=2,aes(x=h2o.fpr(RFt,h2o.find_threshold_by_max_metric(RFt,"f2"))[[1]],y=h2o.tpr(RFt,h2o.find_threshold_by_max_metric(RFt,"f2"))[[1]]),col="blue")

```

```{r fig.cap="F2-Threshold curve and ROC curve for the **validation set**", cache=FALSE}
grid.arrange(RFev,RFav,ncol=1)
```
```{r fig.cap="F2-Threshold curve and ROC curve for the **test set**", cache=FALSE}
grid.arrange(RFet,RFat,ncol=1)
```
    
The confusion matrices are as follows:  
**Validation set**:  
```{r cache=FALSE}
pander(h2o.confusionMatrix(RFp,metrics="f2")[,1:3])
```

**Test set**:  
```{r cache=FALSE}
pander(h2o.confusionMatrix(RFt,metrics="f2")[,1:3])
```
  
We failed to identify `r round(h2o.confusionMatrix(RFt,metrics="f2")[2,3]*100,2)`% of the credit default in the test set which is higher than the NB estimator, and the class error of not defaulting is `r round(h2o.confusionMatrix(RFt,metrics="f2")[1,3]*100,2)`%, which is lower than the NB estimator.
  
The following graph shows the relative importance of the variables for random forest. 
```{r cache=FALSE, fig.cap="Relative strength of 10 most important variables"}
ggplot(data.table(cbind(h2o.varimp(RFb)$variable[1:10],h2o.varimp(RFb)$scaled_importance[1:10],h2o.varimp(RFb)$relative_importance[1:10])))+
  geom_col(aes(x=V1,y=as.numeric(V2),fill=as.numeric(V3)))+coord_flip()+scale_x_discrete(limits=rev(h2o.varimp(RFb)$variable[1:10]))+
  scale_y_continuous(breaks=seq(0,1,0.25))+
  theme(axis.ticks=element_blank())+ylab("Relative Importance")+xlab("Variable")+
  scale_fill_distiller(palette="Spectral",guide=F)
```
  
Payment status are the most important variables but this time age, log credit limit and credit limit are also important to the prediction.  
  
### d. Gradient Boosting Machine  
  
Here I try to apply GBM, I defined a grid **GBM** in H2O, then run grid search using different max tree depth and learning rate, then I choose the best model with highest F2 score.  
```{r results="hide", message=FALSE, warning=FALSE,cache=FALSE}
h2o.rm("GBM")

GBM<-h2o.grid(
  algorithm = "gbm", 
  grid_id = "GBM",
  hyper_params = list(learn_rate=c(0.06,0.07,0.08,0.09,0.1),max_depth=c(4,5,6,7)),
  training_frame = h2o_sd$train,
  validation_frame = h2o_sd$valid,
  x=colnames(h2o_sd$train)[-24],
  y="y",
  seed=0706,
  ntrees=100
)
GBMm<-h2o.getGrid(
  grid_id = "GBM", 
  sort_by = "F2",
  decreasing = TRUE
)

GBMb<- h2o.getModel(GBMm@model_ids[[1]])
```
  
The best model is the model with `r GBMb@parameters$max_depth` as max tree depth for each tree and `r GBMb@parameters$learn_rate` as learning rate.  
  
I then extract the performance and do prediction on the test set.  
```{r message=FALSE, warning=FALSE, results="hide",cache=FALSE}
GBMp<-h2o.performance(GBMb,valid=T)
h2o.auc(GBMp)
GBMrp<-cbind(h2o.fpr(GBMp),h2o.tpr(GBMp)$tpr)
colnames(GBMrp)[3]<-"tpr"
GBMt<-h2o.performance(GBMb,newdata = h2o_sd$test)
GBMrt<-cbind(h2o.fpr(GBMt),h2o.tpr(GBMt)$tpr)
h2o.auc(GBMt)
colnames(GBMrt)[3]<-"tpr"
```
  
I get an AUC of `r round(h2o.auc(GBMp),4)` for the validation set and an AUC of `r round(h2o.auc(GBMt),4)` for the test set, which are both better than the NB predictor, but worse than GLM and random forest. The following two graphs are the F2 to threshold curve and the ROC curve, the blue dots again indicate the threshold chosen.  

```{r, cache=FALSE}
GBMev<-ggplot(h2o.F2(GBMp))+geom_line(aes(x=threshold,y=f2,color=threshold),size=1)+
  scale_color_gradient2("Threshold",low="red",high="green",mid="yellow",midpoint = 0.5)+
  xlab("Threshold")+ylab("F2 Score")+xlim(0,1)+
  geom_point(size=2,aes(x=h2o.find_threshold_by_max_metric(GBMp,"f2"),y=h2o.F2(GBMp,h2o.find_threshold_by_max_metric(GBMp,"f2"))[[1]]),col="blue")
GBMet<-ggplot(h2o.F2(GBMt))+geom_line(aes(x=threshold,y=f2,color=threshold),size=1)+
  scale_color_gradient2("Threshold",low="red",high="green",mid="yellow",midpoint = 0.5)+
  xlab("Threshold")+ylab("F2 Score")+xlim(0,1)+
  geom_point(size=2,aes(x=h2o.find_threshold_by_max_metric(GBMt,"f2"),y=h2o.F2(GBMt,h2o.find_threshold_by_max_metric(GBMt,"f2"))[[1]]),col="blue")
GBMav<-ggplot(GBMrp,aes(x=fpr,y=tpr))+geom_line(aes(col=threshold),size=1)+xlab("False Positive Rate")+ylab("True Positive Rate")+
  scale_color_gradient2("Threshold",low="red",high="green",mid="yellow",midpoint = 0.5)+
  geom_segment(x=0,y=0,xend=1,yend=1,size=1,col="#00BFC4")+xlim(0,1)+ylim(0,1)+
  geom_point(size=2,aes(x=h2o.fpr(GBMp,h2o.find_threshold_by_max_metric(GBMp,"f2"))[[1]],y=h2o.tpr(GBMp,h2o.find_threshold_by_max_metric(GBMp,"f2"))[[1]]),col="blue")
GBMat<-ggplot(GBMrt,aes(x=fpr,y=tpr))+geom_line(aes(col=threshold),size=1)+xlab("False Positive Rate")+ylab("True Positive Rate")+
  scale_color_gradient2("Threshold",low="red",high="green",mid="yellow",midpoint = 0.5)+
  geom_segment(x=0,y=0,xend=1,yend=1,size=1,col="#00BFC4")+xlim(0,1)+ylim(0,1)+
  geom_point(size=2,aes(x=h2o.fpr(GBMt,h2o.find_threshold_by_max_metric(GBMt,"f2"))[[1]],y=h2o.tpr(GBMt,h2o.find_threshold_by_max_metric(GBMt,"f2"))[[1]]),col="blue")

```

```{r fig.cap="F2-Threshold curve and ROC curve for the **validation set**", cache=FALSE}
grid.arrange(GBMev,GBMav,ncol=1)
```
```{r fig.cap="F2-Threshold curve and ROC curve for the **test set**", cache=FALSE}
grid.arrange(GBMet,GBMat,ncol=1)
```
    
The confusion matrices are as follows:  
**Validation set**:  
```{r cache=FALSE}
pander(h2o.confusionMatrix(GBMp,metrics="f2")[,1:3])
```

**Test set**:  
```{r cache=FALSE}
pander(h2o.confusionMatrix(GBMt,metrics="f2")[,1:3])
```
  
We failed to identify `r round(h2o.confusionMatrix(GBMt,metrics="f2")[2,3]*100,2)`% of the credit default in the test set and the class error of not defaulting is `r round(h2o.confusionMatrix(GBMt,metrics="f2")[1,3]*100,2)`%, which are both lower than the NB estimator.
  
The following graph shows the relative importance of the variables for GBM. 
```{r cache=FALSE, fig.cap="Relative strength of 10 most important variables"}
ggplot(data.table(cbind(h2o.varimp(GBMb)$variable[1:10],h2o.varimp(GBMb)$scaled_importance[1:10],h2o.varimp(GBMb)$relative_importance[1:10])))+
  geom_col(aes(x=V1,y=as.numeric(V2),fill=as.numeric(V3)))+coord_flip()+scale_x_discrete(limits=rev(h2o.varimp(GBMb)$variable[1:10]))+
  scale_y_continuous(breaks=seq(0,1,0.25))+
  theme(axis.ticks=element_blank())+ylab("Relative Importance")+xlab("Variable")+
  scale_fill_distiller(palette="Spectral",guide=F)
```
  
Payment status are the most important variables, especially the latest payment status. Besides, age, log credit limit, log payment amount and credit limit are also important to the prediction.  
  
### d. Ensemble  
  
I decided to try to improve by model a bit by using a ensemble with my three models, I am not going to interpret any results here, but the metrics are going to be discussed in the following part for comparison purpose.  
What I have done here is I take the predictions from GLM, RF and GBM and put them in a Naive Bayes model, where y is still the response variable and using m1, m2 and m3 as explanatory variables, which are the predictions from the models described above.     
```{r results="hide", warning=F,message=F,cache=F}
m1p<-as.vector(h2o.predict(GLMb,newdata = h2o_sd$test)$p1)
m2p<-as.vector(h2o.predict(RFb,newdata = h2o_sd$test)$p1)
m3p<-as.vector(h2o.predict(GBMb,newdata = h2o_sd$test)$p1)
tsset<-data.table(as.data.frame(h2o_sd$test))
tsset[,m1:=m1p]
tsset[,m2:=m2p]
tsset[,m3:=m3p]
h2o_ts<-as.h2o(tsset)
h2o_sd2<- h2o.splitFrame(h2o_ts, ratios = c(.6, 0.2),seed=0706)
names(h2o_sd2) <- c("train", "valid", "test")
EN<-h2o.naiveBayes(
  training_frame = h2o_sd2$train,
  validation_frame = h2o_sd2$valid,
  x=c("m1","m2","m3"),
  y="y",
  seed=0706,
  nfolds = 10
)
ENt<-h2o.performance(EN,newdata = h2o_sd2$test)
```

## 4. Results and Conclusion  
  
After running all models, I summarize AUC, F2 Score and Classification error for test set for all results in the following table.  
  
| Metrics | NB | GLM | RF | GBM | EN | 
|---|:----:|:----:|:----:|:----:|:----:|
|AUC|`r round(as.numeric(h2o.auc(NBt)),4)`|`r round(as.numeric(h2o.auc(GLMt)),4)`|`r round(as.numeric(h2o.auc(RFt)),4)`|`r round(as.numeric(h2o.auc(GBMt)),4)`|`r round(as.numeric(h2o.auc(ENt)),4)`| 
|F2|`r round(as.numeric(h2o.F2(NBt,thresholds = h2o.find_threshold_by_max_metric(NBt,metric="f2"))),4)`|`r round(as.numeric(h2o.F2(GLMt,thresholds = h2o.find_threshold_by_max_metric(GLMt,metric="f2"))),4)`|`r round(as.numeric(h2o.F2(RFt,thresholds = h2o.find_threshold_by_max_metric(RFt,metric="f2"))),4)`|`r round(as.numeric(h2o.F2(GBMt,thresholds = h2o.find_threshold_by_max_metric(GBMt,metric="f2"))),4)`|`r round(as.numeric(h2o.F2(ENt,thresholds = h2o.find_threshold_by_max_metric(ENt,metric="f2"))),4)`| 
|Error (FN/P)|`r round(as.numeric(h2o.confusionMatrix(NBt,metrics="f2")[2,3]),4)`|`r round(as.numeric(h2o.confusionMatrix(GLMt,metrics="f2")[2,3]),4)`|`r round(as.numeric(h2o.confusionMatrix(RFt,metrics="f2")[2,3]),4)`|`r round(as.numeric(h2o.confusionMatrix(GBMt,metrics="f2")[2,3]),4)`|`r round(as.numeric(h2o.confusionMatrix(ENt,metrics="f2")[2,3]),4)`| 
|Error (FP/N)|`r round(as.numeric(h2o.confusionMatrix(NBt,metrics="f2")[1,3]),4)`|`r round(as.numeric(h2o.confusionMatrix(GLMt,metrics="f2")[1,3]),4)`|`r round(as.numeric(h2o.confusionMatrix(RFt,metrics="f2")[1,3]),4)`|`r round(as.numeric(h2o.confusionMatrix(GBMt,metrics="f2")[1,3]),4)`|`r round(as.numeric(h2o.confusionMatrix(ENt,metrics="f2")[1,3]),4)`|  
  
Overall I think this machine learning task does not have a very high AUC rate but still successful. All GLM, RF, GBM and their ensemble have a higher AUC than the NB classifier, which is a good sign that my effort actually pays out. Surprisingly, GBM has a lower AUC than GLM and RF even though it should be a more complicated model. I actually planned to apply Artificial Neural Network but it is taking too long to run and its AUC is actually even lower, so it is not really worth it.  
GLM and RF have very similar AUC and F2 even though they are very different models, although their classification errors differ a lot because of using different threshold.
On the other hand, the ensemble of the GLM, RF and GBM model actually has the highest AUC in the test set and also the highest F2 score. It has fair classification errors in both directions compared to other models, so I think the model EN is the best model here.  
One improvement I could make is that I would actually want to use Cross Validation in other models like in EN as it can give us more stable result. All models are giving very similar performance so a more stable result can help us do better decision.  
Another improvement is that I should use more values in grid search to fine tune the parameters, but it is going to take a long time so I will try that later.  

```{r results="hide"}
h2o.shutdown(prompt=F)
```

